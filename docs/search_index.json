[["index.html", "Skin Laundry Technical Assessment 1 Assessment Objective", " Skin Laundry Technical Assessment Austin Kolvek 2025-01-12 1 Assessment Objective The goal of this assessment is to take the data Skin Laundry has provided and build a forecasting model that will predict future demand for their Hyaluronic Acid Serum at clinic G during Q3 of 2024 (July - September 2024). "],["eda.html", "2 EDA", " 2 EDA import pandas as pd data = pd.read_csv(r&#39;C:\\Users\\austi\\OneDrive\\Documents\\Interviews\\Skin Laundry\\Clinic Product Sales Data.csv&#39;) Lets take a look at the first couple of observations! Before bringing the data into Python, I completed some side work in Excel to line up what days had which promotion. #Set the display up so we can see all of the columns pd.set_option(&#39;display.max_columns&#39;, None) print(data.head()) ## clinic area clinic name sale date guest user id \\ ## 0 Region B Clinic G 1/2/2022 2327eb10-952d-431f-8e97-61d37b81944d ## 1 Region B Clinic G 1/2/2022 2327eb10-952d-431f-8e97-61d37b81944d ## 2 Region B Clinic G 1/2/2022 721fe77e-dd3f-4cba-8038-2096b542b9c4 ## 3 Region B Clinic G 1/2/2022 721fe77e-dd3f-4cba-8038-2096b542b9c4 ## 4 Region B Clinic G 1/2/2022 3cd96a41-d65a-42b7-b42e-6e18cf7bdc3b ## ## product name product category quantity sold Promotion ## 0 TA Tranexamic Acid Serum Serum 1 No Promotion ## 1 Hyaluronic Acid Serum Serum 1 No Promotion ## 2 VA Vitamin A Serum Serum 1 No Promotion ## 3 Hyaluronic Acid Serum Serum 1 No Promotion ## 4 Hyaluronic Acid Serum Serum 1 No Promotion Right away, some of the columns that don’t seem to be needed are “guest user id, product category, and clinic area. Lets check the unique values of product category to make sure we can drop it. print(data[&#39;product category&#39;].unique()) ## [&#39;Serum&#39;] “Serum” is the only value for the category, so we are good to drop this column as it provides no info for us. Lets drop the columns we stated above. data = data.drop(columns=[&#39;clinic area&#39;,&#39;guest user id&#39;,&#39;product category&#39;]) print(data.head()) ## clinic name sale date product name quantity sold Promotion ## 0 Clinic G 1/2/2022 TA Tranexamic Acid Serum 1 No Promotion ## 1 Clinic G 1/2/2022 Hyaluronic Acid Serum 1 No Promotion ## 2 Clinic G 1/2/2022 VA Vitamin A Serum 1 No Promotion ## 3 Clinic G 1/2/2022 Hyaluronic Acid Serum 1 No Promotion ## 4 Clinic G 1/2/2022 Hyaluronic Acid Serum 1 No Promotion Next I am going to subset the data down to info that Skin Laundry as specified. subset_data = data[(data[&#39;clinic name&#39;] == &#39;Clinic G&#39;) &amp; (data[&#39;product name&#39;] == &#39;Hyaluronic Acid Serum&#39;)] print(subset_data.head()) ## clinic name sale date product name quantity sold Promotion ## 1 Clinic G 1/2/2022 Hyaluronic Acid Serum 1 No Promotion ## 3 Clinic G 1/2/2022 Hyaluronic Acid Serum 1 No Promotion ## 4 Clinic G 1/2/2022 Hyaluronic Acid Serum 1 No Promotion ## 12 Clinic G 1/4/2022 Hyaluronic Acid Serum 1 No Promotion ## 18 Clinic G 1/8/2022 Hyaluronic Acid Serum 1 No Promotion # Cut the data down to the correct time range from the beginning of 2022 to # September 2024 # Define the start and end date for the time range start_date = &#39;1/2/2022&#39; end_date = &#39;9/30/2024&#39; # Filter the data based on the time range subset_data = subset_data[(subset_data[&#39;sale date&#39;] &gt;= start_date) &amp; (subset_data[&#39;sale date&#39;] &lt;= end_date)] # Sort the data by &#39;sale date&#39; from earliest to most recent subset_data = subset_data.sort_values(&#39;sale date&#39;, ascending=True) Next I am going to make sure the our variables are of correct type. Any variables that are not of the correct type, I will correct accordingly. print(subset_data.info()) ## &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; ## Index: 1353 entries, 1 to 14719 ## Data columns (total 5 columns): ## # Column Non-Null Count Dtype ## --- ------ -------------- ----- ## 0 clinic name 1353 non-null object ## 1 sale date 1353 non-null object ## 2 product name 1353 non-null object ## 3 quantity sold 1353 non-null int64 ## 4 Promotion 1353 non-null object ## dtypes: int64(1), object(4) ## memory usage: 63.4+ KB ## None I am going to change the “sale date” column to a datetime data type. subset_data.loc[:, &#39;sale date&#39;] = pd.to_datetime(subset_data[&#39;sale date&#39;]) Next we are going to check fo NA values. print(subset_data.isna().sum()) ## clinic name 0 ## sale date 0 ## product name 0 ## quantity sold 0 ## Promotion 0 ## dtype: int64 We have 0 NA values! Lets count how many Hyaluronic Acid Serums clinic G has sold in total. print(subset_data.groupby(&#39;product name&#39;)[&#39;quantity sold&#39;].sum()) ## product name ## Hyaluronic Acid Serum 1394 ## Name: quantity sold, dtype: int64 Clinic G for Skin Laundry has sold a total of 1,394 Hyaluronic Acids Serums from January 2022-September 2024! I am going to save the clean data and pull it into R to build my time series model. subset_data.to_csv(&#39;subset_data.csv&#39;, index=False) Moving forward, I will build a time series forecast that will look at the patterns in the data to predict demand for the Hyaluronic Acid Serum in Q3 of 2024. "],["modeling.html", "3 Modeling", " 3 Modeling Lets bring in the data from the EDA and the promotion data! data &lt;- read.csv(&#39;C:\\\\Users\\\\austi\\\\OneDrive\\\\Documents\\\\skinlaundryapp\\\\subset_data.csv&#39;) promo_data &lt;- read.csv(&#39;C:\\\\Users\\\\austi\\\\OneDrive\\\\Documents\\\\Interviews\\\\Skin Laundry\\\\Promotion details and dates.csv&#39;) Check to make sure the data looks good. library(lubridate) library(fpp3) str(data) ## &#39;data.frame&#39;: 1353 obs. of 5 variables: ## $ clinic.name : chr &quot;Clinic G&quot; &quot;Clinic G&quot; &quot;Clinic G&quot; &quot;Clinic G&quot; ... ## $ sale.date : chr &quot;2022-01-02 00:00:00&quot; &quot;2022-01-02 00:00:00&quot; &quot;2022-01-02 00:00:00&quot; &quot;2023-01-02 00:00:00&quot; ... ## $ product.name : chr &quot;Hyaluronic Acid Serum&quot; &quot;Hyaluronic Acid Serum&quot; &quot;Hyaluronic Acid Serum&quot; &quot;Hyaluronic Acid Serum&quot; ... ## $ quantity.sold: int 1 1 1 1 1 1 1 1 1 1 ... ## $ Promotion : chr &quot;No Promotion&quot; &quot;No Promotion&quot; &quot;No Promotion&quot; &quot;No Promotion&quot; ... #There needs to get one observation per day data &lt;- data %&gt;% group_by(sale.date, clinic.name, product.name,Promotion) %&gt;% summarize(total_quantity_sold = sum(`quantity.sold`), .groups = &#39;drop&#39;) To build a time series model, I am going to convert the data into a tsibble. This gets our data into the correct format and makes model building much easier. data&lt;- data %&gt;% mutate(sale.date = as.Date(sale.date)) %&gt;% as_tsibble(index = sale.date)%&gt;% fill_gaps() %&gt;% mutate(total_quantity_sold = ifelse(is.na(total_quantity_sold), 0, total_quantity_sold)) %&gt;% mutate(Promotion = ifelse(is.na(Promotion),&quot;No Promotion&quot;, Promotion)) %&gt;% mutate(clinic.name = ifelse(is.na(clinic.name),&quot;Clinic G&quot;, clinic.name)) %&gt;% mutate(product.name = ifelse(is.na(product.name),&quot;Hyaluronic Acid Serum&quot;, product.name)) I am going to split the data into a training and test set. The training set will be all the data up until July of 2024 and the test set will be all of the data from July 2024-September 2024. train &lt;- data %&gt;% select(total_quantity_sold, sale.date, Promotion) %&gt;% filter_index(~ &quot;2024-06-30&quot;) test &lt;- data %&gt;% select(total_quantity_sold, sale.date, Promotion) %&gt;% filter_index(&quot;2024-07-01&quot; ~ &quot;2024-09-30&quot;) Lets get a basic look for what the data looks like over time. library(scales) autoplot(train, total_quantity_sold) + geom_line(aes(y = total_quantity_sold)) + geom_point(aes(color = Promotion)) + scale_x_date( breaks = &quot;1 month&quot;, # Set the breaks to monthly intervals labels = date_format(&quot;%b %Y&quot;) # Format the labels as Month-Year ) + theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) This is a very messy graph but a few points are good to point out. When there is a spike in sales for the product, there seems to be a promotion attached to the spike. It is note worthy to point out the 30% off all products, 30% off product, and Buy 2 Get Cleanser free are the promotions that have the biggest spikes in the data. On the flip side, the Spend more Save more, Buy 3 Get 1 free, and 10% off serums don’t seem to boost sales for this specific product. I am going to create a STL Decomposition to decide if the data has seasonality or a trend to it. stl &lt;- train %&gt;% model(stl = STL(total_quantity_sold)) components(stl) ## # A dable: 911 x 8 [1D] ## # Key: .model [1] ## # : total_quantity_sold = trend + season_year + season_week + remainder ## .model sale.date total_quantity_sold trend season_week season_year remainder season_adjust ## &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 stl 2022-01-02 3 1.10 0.579 0.293 1.03 2.13 ## 2 stl 2022-01-03 0 1.10 0.368 -0.684 -0.787 0.316 ## 3 stl 2022-01-04 1 1.10 -0.391 0.884 -0.597 0.506 ## 4 stl 2022-01-05 0 1.10 -0.296 0.134 -0.940 0.163 ## 5 stl 2022-01-06 0 1.10 0.219 -1.25 -0.0710 1.03 ## 6 stl 2022-01-07 0 1.10 -0.0131 -0.707 -0.383 0.720 ## 7 stl 2022-01-08 1 1.10 -0.460 0.459 -0.102 1.00 ## 8 stl 2022-01-09 0 1.10 0.561 -0.710 -0.954 0.149 ## 9 stl 2022-01-10 0 1.10 0.391 -1.34 -0.154 0.949 ## 10 stl 2022-01-11 0 1.10 -0.407 -1.14 0.448 1.55 ## # ℹ 901 more rows components(stl) %&gt;% autoplot() + theme_classic() train %&gt;% features(total_quantity_sold, feat_stl) ## # A tibble: 1 × 9 ## trend_strength seasonal_strength_week seasonal_peak_week seasonal_trough_week spikiness linearity curvature stl_e_acf1 stl_e_acf10 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.360 0.273 6 1 0.000147 7.03 4.91 -0.0497 0.224 #trend = 0.3596 #seasonality = 0.2737 There is not enough evidence to suggest there to be a trend or seasonality in the data. The next step is to figure out if the data is stationary. I am going to perform a KPSS test to determine if there is stationarity or not. #What should the alpha level be? pchisq(log(909),1,lower.tail = F) ## [1] 0.00905298 #alpha level = 0.00905298 train %&gt;% features(total_quantity_sold, unitroot_kpss) ## # A tibble: 1 × 2 ## kpss_stat kpss_pvalue ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.892 0.01 Since our p-value is to high, this suggest that the data is a random walk. It is not possible to forecast a random walk so in order to combat this, I am going to difference the quantity sold variable(The response variable). This will let us be able to create a forecast. train &lt;- train %&gt;% mutate(diff_quantity = difference(total_quantity_sold)) Next I am going to look at a Auto Correlation(ACF) and Partial Auto Correlation (PACF) plot to determine the patterns in the data. train %&gt;% gg_tsdisplay(diff_quantity, plot_type = &#39;partial&#39;) ## Warning: Removed 1 row containing missing values or values outside the scale range (`geom_line()`). ## Warning: Removed 1 row containing missing values or values outside the scale range (`geom_point()`). Notes from the ACF and PACF plot: To me, it looks like there is an exponential decrease in the pacf plot. This suggests that there are no auto regressive(AR) terms in the data and we should only be looking at how many moving average(MA) terms to add to our model. With the acf plot, there is a clear big spike at lag 1 and then a consistent hovering pattern around the confidence interval. This suggest there is one MA term that needs to be in the model. Moving forward, I will create a handful of different models to see which one works the best. Based on the AIC and BIC selection criteria, I will decide which model to move forward with. models &lt;- train %&gt;% model( auto = ARIMA(diff_quantity), step = ARIMA(diff_quantity, stepwise = FALSE), MA1 = ARIMA(diff_quantity ~ 0 + pdq(0,1,1) + PDQ(0,0,0)), MA7 = ARIMA(diff_quantity ~ 0 + pdq(0,1,7) + PDQ(0,0,0)), ARIMA91 = ARIMA(diff_quantity ~ 0 + pdq(9,1,1) + PDQ(0,0,0)) ) model_results &lt;- as.data.frame(models) glance(models) ## # A tibble: 5 × 8 ## .model sigma2 log_lik AIC AICc BIC ar_roots ma_roots ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;list&gt; &lt;list&gt; ## 1 auto 3.39 -1844. 3701. 3701. 3730. &lt;cpl [5]&gt; &lt;cpl [0]&gt; ## 2 step 3.39 -1844. 3701. 3701. 3730. &lt;cpl [5]&gt; &lt;cpl [0]&gt; ## 3 MA1 5.06 -2030. 4063. 4063. 4073. &lt;cpl [0]&gt; &lt;cpl [1]&gt; ## 4 MA7 3.11 -1810. 3637. 3637. 3675. &lt;cpl [0]&gt; &lt;cpl [7]&gt; ## 5 ARIMA91 3.14 -1811. 3643. 3644. 3696. &lt;cpl [9]&gt; &lt;cpl [1]&gt; models %&gt;% select(ARIMA91) %&gt;% gg_tsresiduals() ## Warning: Removed 1 row containing missing values or values outside the scale range (`geom_line()`). ## Warning: Removed 1 row containing missing values or values outside the scale range (`geom_point()`). ## Warning: Removed 1 row containing non-finite outside the scale range (`stat_bin()`). augment(models) %&gt;% filter(.model == &#39;ARIMA91&#39;) %&gt;% features(.innov, ljung_box, lag = 12, dof = 10) ## # A tibble: 1 × 3 ## .model lb_stat lb_pvalue ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 ARIMA91 11.2 0.00363 For the ARIMA models, I decided to pick a model with 9 AR terms, 1 MA terms, and took a difference. I choose this setup by looking at the ACF and PACF plots. The PACF plot looked liked it had an exponential decrease in it. Apon further examination at the tail end of the plot, the 20th and 27th lags show signs of being included in the model which tells me there needs to be AR terms in the model. After building the model, I looked at the residuals to determine if white noise was left over. I assessed white noise in two ways. The first was looking at the residual plot. I am comfortable with the distribution of the residuals being normal and the ACF plot being close to the bounds of the confidence interval. The second was performing a Lijung Box test. With a p-value of 0.003 and the alpha level being 0.008, this tells me there is a little bit of noise that can still be modeled. When looking at the original plot, there was a point intervention that stood out to me. By including this point into the model, I think it might reduce the noise in the model to only have white noise left over. #Point intervention 692 # Add a point intervention column to both the Train AND Test sets train$point &lt;- rep(0,911) test$point &lt;- rep(0,92) train$point[692] &lt;- 1 # Fit ARIMA model with intervention models_point &lt;- train %&gt;% model( point_intervention = ARIMA(diff_quantity ~ point + lag(point) + 0 + pdq(9,1,1) + PDQ(0,0,0)) ) # Glance at model summary glance(models_point) ## # A tibble: 1 × 8 ## .model sigma2 log_lik AIC AICc BIC ar_roots ma_roots ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;list&gt; &lt;list&gt; ## 1 point_intervention 2.39 -1687. 3400. 3400. 3462. &lt;cpl [9]&gt; &lt;cpl [1]&gt; # Visualize residuals of the point intervention model models_point %&gt;% select(point_intervention) %&gt;% gg_tsresiduals() ## Warning: Removed 1 row containing missing values or values outside the scale range (`geom_line()`). ## Warning: Removed 1 row containing missing values or values outside the scale range (`geom_point()`). ## Warning: Removed 1 row containing non-finite outside the scale range (`stat_bin()`). # Ljung-Box test for autocorrelation in the residuals augment(models_point) %&gt;% filter(.model == &#39;point_intervention&#39;) %&gt;% features(.innov, ljung_box, lag = 12, dof = 10) ## # A tibble: 1 × 3 ## .model lb_stat lb_pvalue ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 point_intervention 9.49 0.00868 # Forecasting the next 92 periods beyond the training data and plotting forecast_values_point &lt;- models_point %&gt;% forecast(new_data = test) # Extract forecasted values forecasted_mean &lt;- forecast_values_point$.mean # Ensure your actual values match the forecast horizon length actual_values &lt;- test$total_quantity_sold[1:length(forecasted_mean)] # Calculate residuals (difference between actual and forecasted values) residuals &lt;- actual_values - forecasted_mean # Calculate MAE (Mean Absolute Error) MAE &lt;- mean(abs(residuals)) MAE ## [1] 1.418334 #MAE = 1.41 When adding a point intervention to the data, the model seems to only have white noise left over. I am comfortable with this conclusion due the residuals looking like a normal distribution, the lags are within the range of the CI and the Lijung Box p-value came back at 0.008 with an alpha level of 0.008. To asses the accuracy of this model, I decided to use Mean Absolute Error(MAE). When using this model to forecast future demand of the Hyulanioc Acid product at clinic G for Q3, this model was 1.4 units off, on average. In the summary section, I will give a more detailed overview wrapping this analysis up and how Skin Laundry can use this model moving forward. "],["summary.html", "4 Summary", " 4 Summary In this analysis, a demand forecasting model was developed for Hyaluronic Acid serum at Clinic G for Q3 of 2024 (July–September). Using an ARIMA model with a point intervention, the model achieved a Mean Absolute Error (MAE) of 1.4, indicating a strong prediction accuracy. This performance provides the clinic with a reliable forecast, helping ensure that inventory is adequately stocked to meet customer demand and reduce the risk of out-of-stock situations during this period. Key Findings and Insights: Demand Forecasting Accuracy: The ARIMA model’s MAE of 1.4 suggests that the forecast is precise and can be trusted to guide inventory planning and ordering decisions for Hyaluronic Acid serum. This will be critical for ensuring that Clinic G can maintain stock levels that align with anticipated demand, thus avoiding overstocking or stockouts and optimizing warehouse operations. Promotions and Sales Patterns: An important insight was discovered when plotting the historical sales data alongside promotional periods. The data revealed that certain promotions influence product sales, while others do not. Specifically: Promotions with impact on sales: -&gt; 30% off all products -&gt; 30% off product -&gt; Buy 2 Get Cleanser free These promotions caused substantial spikes in sales, suggesting that Clinic G can strategically use these promotions to boost demand and drive sales for the Hyaluronic Acid serum. Promotions with minimal impact: -&gt; Spend more Save more -&gt; Buy 3 Get 1 free -&gt; 10% off serums These promotions did not produce noticeable increases in sales, indicating that they might not be as effective for this particular product. Business Implications: Inventory Management: With the forecasted demand in hand and an understanding of the promotional impact, Clinic G can optimize their ordering and stock levels. This ensures that the clinic is not overstocked during periods of low demand or understocked during promotional spikes, which is essential for maintaining customer satisfaction and maximizing sales. Promotion Strategy: The insights into promotional effectiveness offer valuable guidance for Clinic G’s future marketing strategies. By focusing on promotions that have historically proven successful, such as the “30% off all products” or “Buy 2 Get Cleanser free,” Clinic G can more effectively drive sales of the Hyaluronic Acid serum and achieve better return on investment from their marketing efforts. Additionally, reducing reliance on less effective promotions can help save resources and improve marketing efficiency. Forecast Accuracy for Planning: The ARIMA model’s accuracy in predicting the demand for Q3 2024 can serve as a reliable basis for other product demand forecasts. If the clinic utilizes similar forecasting techniques across other products, it will help to streamline their overall inventory management and improve operational efficiency. Conclusion: The demand forecasting model provides valuable insight into the future sales of Hyaluronic Acid serum at Clinic G, ensuring that inventory planning is accurate and efficient. By strategically aligning promotional efforts with the most impactful offers, Clinic G can boost sales and avoid unnecessary stockouts. Overall, these insights will help optimize the clinic’s operations, improve customer satisfaction, and drive growth in the coming quarter. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
